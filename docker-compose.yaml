services:
  mlflow:
    build:
      context: ./build/mlflow_build
      dockerfile: Dockerfile
    image: mlflow_server:1.0
    depends_on:
      - postgres
    ports:
      - "8080:5000"
    environment:
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: 23wesdxc
      MLFLOW_TRACKING_URI: "http://localhost:8080"
    entrypoint: bash -c "mlflow server --backend-store-uri postgresql://postgres:postgres@postgres:5432/postgres --host 0.0.0.0 --port 5000"

  postgres:
    image: postgres:16.3-alpine
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres

  # Тритон для инференса моделей
  triton:
    container_name: triton
    image: nvcr.io/nvidia/tritonserver:24.09-py3
    ports:
      - 8000:8000  # HTTP endpoint for inference
      - 8001:8001  # GRPC endpoint for inference
      - 8002:8002  # Metrics endpoint for Prometheus
    restart: always
    volumes:
      - ./triton/models:/models
    command: ["tritonserver", "--model-store=/models"]
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
    #           count: 1
